{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8차시: 텐서플로우 2.x 활용 Speech Recognition\n",
    "\n",
    "## AI 맛보기 8주차: 2020. 08. 25. 20:00 ~ 22:00 (120분)\n",
    "1. 도구 불러오기 및 버전 확인\n",
    "1. 학습 데이터 다운로드 및 압축 풀기\n",
    "1. 학습 데이터 살펴보기: 레이블 확인, 들어보기\n",
    "1. 학습 데이터 전처리: 스펙트로그램\n",
    "1. 학습 모델 준비: CNN\n",
    "1. 학습\n",
    "1. 학습 결과 테스트\n",
    "\n",
    "#### 참고자료\n",
    "- [파이썬 3 표준 문서](https://docs.python.org/3/index.html)\n",
    "- [Audio Spectrogram](https://www.tensorflow.org/io/tutorials/audio#trim_the_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 도구 불러오기 및 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 준비\n",
    "import os\n",
    "# import shutil\n",
    "import random\n",
    "import math\n",
    "\n",
    "import tensorflow as tf # 텐서플로우\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt # 시각화 도구\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tensorflow 버전을 확인합니다: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 학습 데이터 다운로드 및 압축 풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 다운로드\n",
    "- [https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz](https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r data_speech_commands\n",
    "!mkdir data_speech_commands\n",
    "!tar --directory data_speech_commands -xvf data_speech_commands_v0.02.tar.gz &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r data_speech_commands/_background_noise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = './data_speech_commands'\n",
    "\n",
    "files = list()\n",
    "labels = list()\n",
    "lab2idx = list()\n",
    "for l1 in os.scandir(path_root):\n",
    "    if l1.is_file():\n",
    "        continue\n",
    "    label = os.path.basename(l1.path)\n",
    "    if label not in lab2idx:\n",
    "        lab2idx.append(label)\n",
    "    label_idx = lab2idx.index(label)\n",
    "    for l2 in os.scandir(l1.path):\n",
    "        files.append(l2.path)\n",
    "        labels.append([label_idx])\n",
    "dataset_root = (files, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(files, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 학습 데이터 살펴보기: 차원, 레이블, 듣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = tfio.audio.AudioIOTensor(x_test[0])\n",
    "print(audio)\n",
    "audio_tensor = tf.squeeze(audio.to_tensor(), axis=-1)\n",
    "print(audio_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printed = set()\n",
    "for data, label in zip(x_test, y_test):\n",
    "    label = label[0]\n",
    "    if label in printed:\n",
    "        continue\n",
    "    print(lab2idx[label], end=': ')\n",
    "    audio = tfio.audio.AudioIOTensor(data)\n",
    "    audio_tensor = tf.squeeze(audio.to_tensor(), axis=-1)\n",
    "    display(Audio(audio_tensor.numpy(), rate=audio.rate.numpy()))\n",
    "    printed.add(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 학습 데이터 전처리: 오디오 데이터, 스펙트로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = 1.0     # range [0.0, 1.0]\n",
    "fs = 16000       # sampling rate, Hz, must be integer\n",
    "duration = 1.0   # in seconds, may be float\n",
    "f = 261.625      # sine frequency, Hz, may be float\n",
    "\n",
    "samples = (np.sin(2*np.pi*np.arange(fs*duration)*f/fs)).astype(np.float32)\n",
    "display(Audio(samples, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to spectrogram\n",
    "spectrogram = get_spectrogram(samples)\n",
    "spectrogram = tf.squeeze(spectrogram, -1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.axis('off')\n",
    "_ = ax.imshow(tf.math.log(spectrogram.numpy()), cmap='gray')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_ylim((-1*np.max(samples), np.max(samples)))\n",
    "_ = ax.plot(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_root = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset_root = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tf.cast(audio_tensor, tf.float32) / 2**16\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.set_ylim((-1*np.max(tensor), np.max(tensor)))\n",
    "_ = ax.plot(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(values):\n",
    "    spectrogram = tfio.experimental.audio.spectrogram(tf.convert_to_tensor(values, dtype=tf.float32), \n",
    "                                                      nfft=512, window=512, stride=256)\n",
    "    spectrogram = tf.transpose(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, -1)\n",
    "    spectrogram = tf.image.flip_up_down(spectrogram)\n",
    "    \n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, label):\n",
    "    audio = tfio.audio.AudioIOTensor(path, dtype='int16')\n",
    "    audio_tensor = tf.squeeze(audio.to_tensor(), axis=[-1])\n",
    "    audio_tensor = tf.pad(tf.expand_dims(audio_tensor, 0), ((0, 0), (0, 16000)), \n",
    "                          'constant', constant_values=0)\n",
    "    audio_tensor = tf.slice(audio_tensor, (0, 0), (1, 16000))\n",
    "    audio_tensor = tf.squeeze(audio_tensor, axis=0)\n",
    "    tensor = tf.cast(audio_tensor, tf.float32) / 2**16\n",
    "    \n",
    "    spectrogram = get_spectrogram(tensor)\n",
    "    return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = get_spectrogram(audio_tensor.numpy())\n",
    "spectrogram = tf.squeeze(spectrogram, -1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.axis('off')\n",
    "_ = ax.imshow(tf.math.log(spectrogram.numpy()), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 학습 모델 준비: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset_root.map(load_audio)\n",
    "val_dataset = val_dataset_root.map(load_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=get_spectrogram(samples).shape),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(lab2idx))\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 학습 결과 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['sparse_categorical_accuracy']\n",
    "val_acc = history_dict['val_sparse_categorical_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "fig1 = plt.figure(figsize=(6, 10))\n",
    "ax = fig1.add_subplot(2, 1, 1)\n",
    "ax.plot(epochs, loss, 'bo', label='Training loss')\n",
    "ax.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "ax.set_ylim((0, math.ceil(max(max(loss), max(val_loss)))))\n",
    "ax.set_title('Training and validation loss', fontsize=12)\n",
    "ax.set_xlabel('Epochs', fontsize=10)\n",
    "ax.set_ylabel('Loss', fontsize=10)\n",
    "ax.legend()\n",
    "\n",
    "ax = fig1.add_subplot(2, 1, 2)\n",
    "ax.plot(epochs, acc, 'bo', label='Training acc')\n",
    "ax.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "ax.set_ylim((0, math.ceil(max(max(acc), max(val_acc)))))\n",
    "ax.set_title('Training and validation accuracy', fontsize=12)\n",
    "ax.set_xlabel('Epochs', fontsize=10)\n",
    "ax.set_ylabel('Accuracy', fontsize=10)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'레이블 종류: ')\n",
    "for n, label in enumerate(lab2idx, start=1):\n",
    "    print(label, end=' ')\n",
    "    if n % 10 == 0:\n",
    "        print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = 'happy'\n",
    "test_idx = lab2idx.index(test_label)\n",
    "\n",
    "target = list()\n",
    "for path, label in zip(x_test, y_test):\n",
    "    label = label[0]\n",
    "    if label == test_idx:\n",
    "        target.append((path, label))\n",
    "\n",
    "choice = random.choice(target)\n",
    "test_path = choice[0]\n",
    "\n",
    "audio = tfio.audio.AudioIOTensor(test_path)\n",
    "audio_tensor = tf.squeeze(audio.to_tensor(), axis=-1)\n",
    "print(f'Label: {test_label} [{test_idx}]')\n",
    "display(Audio(audio_tensor.numpy(), rate=audio.rate.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = get_spectrogram(audio_tensor.numpy())\n",
    "spectrogram_ = tf.squeeze(spectrogram, -1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.axis('off')\n",
    "_ = ax.imshow(tf.math.log(spectrogram_.numpy()), cmap='gray')\n",
    "\n",
    "print(f'예측 레이블: {lab2idx[np.argmax(model.predict(tf.expand_dims(spectrogram, 0)))]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
