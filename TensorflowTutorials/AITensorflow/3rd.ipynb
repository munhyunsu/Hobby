{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3차시: 텐서플로우 2.x 활용 감성 분석 기초\n",
    "\n",
    "## AI 맛보기 3주차: 2020. 07. 21. 20:00 ~ 22:00 (120분)\n",
    "1. 도구 불러오기 및 버전 확인\n",
    "1. 학습 데이터 다운로드: 스탠포드 AI 연구실 - 영화 리뷰 DB\n",
    "1. 학습 데이터 살펴보기: 차원, 미리보기\n",
    "1. 학습 데이터 준비: 불러오기\n",
    "1. 학습 데이터 준비: 전처리\n",
    "1. 학습 데이터 준비: 전처리 결과 확인\n",
    "1. 학습 모델 정의 및 생성\n",
    "1. 학습\n",
    "1. 학습 결과 확인\n",
    "1. 예측: 가지고 놀기! \n",
    "\n",
    "#### 참고자료\n",
    "- [파이썬 3 표준 문서](https://docs.python.org/3/index.html)\n",
    "- [텐서플로우 텍스트 분류](https://www.tensorflow.org/tutorials/keras/text_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 도구 불러오기 및 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 준비\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf # 텐서플로우\n",
    "import matplotlib.pyplot as plt # 시각화 도구\n",
    "%matplotlib inline\n",
    "import matplotlib.font_manager as fm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tensorflow 버전을 확인합니다: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 한글 설정\n",
    "fonts = fm.findSystemFonts()\n",
    "nanum_path = None\n",
    "for font in fonts:\n",
    "    if font.endswith('NanumGothic.ttf'):\n",
    "        nanum_path = font\n",
    "        break\n",
    "if nanum_path == None:\n",
    "    print(f'나눔 폰트를 설치해야 합니다!')\n",
    "    print(f'!apt install -qq -y fonts-nanum*')\n",
    "else:\n",
    "    print(f'나눔 폰트 경로: {nanum_path}')\n",
    "    nanum_prop = fm.FontProperties(fname=nanum_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 학습 데이터 다운로드: (aclImdb) 영화 리뷰 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운로드\n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "print(f'데이터 압축 디렉터리 내부: {os.listdir(dataset_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 구조 확인\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "print(f'학습 데이터 디렉터리 내부: {os.listdir(train_dir)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 학습 데이터 살펴보기: 미리보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긍정 학습 데이터 파일 확인\n",
    "pos_train_dir = os.path.join(train_dir, 'pos')\n",
    "pos_train_files = os.listdir(pos_train_dir)\n",
    "neg_train_dir = os.path.join(train_dir, 'neg')\n",
    "neg_train_files = os.listdir(neg_train_dir)\n",
    "print('긍정 파일: ', end='')\n",
    "for _ in range(0, 10):\n",
    "    print(random.choice(pos_train_files), end=' ')\n",
    "print()\n",
    "print('부정 파일: ', end='')\n",
    "for _ in range(0, 10):\n",
    "    print(random.choice(neg_train_files), end=' ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 내용 확인\n",
    "## https://translate.google.co.kr/\n",
    "## https://papago.naver.com/\n",
    "sample_file = os.path.join(train_dir, 'pos', '8740_8.txt')\n",
    "with open(sample_file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 학습 데이터 준비: 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정리\n",
    "print('''학습을 위한 데이터는 이런 구조가 되어야 합니다.\n",
    "train/\n",
    "....pos/\n",
    "........file1.txt\n",
    "........file2.txt\n",
    "....neg/\n",
    "........file1.txt\n",
    "........file2.txt''')\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "print(f'불필요한 데이터 파일을 정리합니다. {remove_dir}')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본: 텐서플로우 Github\n",
    "## https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/preprocessing/text_dataset.py\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "from tensorflow.python.ops import io_ops\n",
    "from tensorflow.python.ops import string_ops\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "def index_directory(directory,\n",
    "                    labels,\n",
    "                    formats,\n",
    "                    class_names=None,\n",
    "                    shuffle=True,\n",
    "                    seed=None,\n",
    "                    follow_links=False):\n",
    "    inferred_class_names = []\n",
    "    for subdir in sorted(os.listdir(directory)):\n",
    "        if os.path.isdir(os.path.join(directory, subdir)):\n",
    "            inferred_class_names.append(subdir)\n",
    "    if not class_names:\n",
    "        class_names = inferred_class_names\n",
    "    else:\n",
    "        if set(class_names) != set(inferred_class_names):\n",
    "            raise ValueError(\n",
    "                    'The `class_names` passed did not match the '\n",
    "                    'names of the subdirectories of the target directory. '\n",
    "                    'Expected: %s, but received: %s' %\n",
    "                    (inferred_class_names, class_names))\n",
    "    class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "\n",
    "    pool = multiprocessing.pool.ThreadPool()\n",
    "    results = []\n",
    "    filenames = []\n",
    "    for dirpath in (os.path.join(directory, subdir) for subdir in class_names):\n",
    "        results.append(\n",
    "            pool.apply_async(index_subdirectory,\n",
    "                             (dirpath, class_indices, follow_links, formats)))\n",
    "    labels_list = []\n",
    "    for res in results:\n",
    "        partial_filenames, partial_labels = res.get()\n",
    "        labels_list.append(partial_labels)\n",
    "        filenames += partial_filenames\n",
    "    if labels != 'inferred':\n",
    "        if len(labels) != len(filenames):\n",
    "            raise ValueError('Expected the lengths of `labels` to match the number '\n",
    "                             'of files in the target directory. len(labels) is %s '\n",
    "                             'while we found %s files in %s.' % (\n",
    "                                     len(labels), len(filenames), directory))\n",
    "    else:\n",
    "        i = 0\n",
    "        labels = np.zeros((len(filenames),), dtype='int32')\n",
    "        for partial_labels in labels_list:\n",
    "            labels[i:i + len(partial_labels)] = partial_labels\n",
    "            i += len(partial_labels)\n",
    "\n",
    "    print('Found %d files belonging to %d classes.' %\n",
    "          (len(filenames), len(class_names)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    file_paths = [os.path.join(directory, fname) for fname in filenames]\n",
    "\n",
    "    if shuffle:\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(1e6)\n",
    "        rng = np.random.RandomState(seed)\n",
    "        rng.shuffle(file_paths)\n",
    "        rng = np.random.RandomState(seed)\n",
    "        rng.shuffle(labels)\n",
    "    return file_paths, labels, class_names\n",
    "\n",
    "\n",
    "def iter_valid_files(directory, follow_links, formats):\n",
    "    walk = os.walk(directory, followlinks=follow_links)\n",
    "    for root, _, files in sorted(walk, key=lambda x: x[0]):\n",
    "        for fname in sorted(files):\n",
    "            if fname.lower().endswith(formats):\n",
    "                yield root, fname\n",
    "        \n",
    "\n",
    "def index_subdirectory(directory, class_indices, follow_links, formats):\n",
    "    dirname = os.path.basename(directory)\n",
    "    valid_files = iter_valid_files(directory, follow_links, formats)\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    for root, fname in valid_files:\n",
    "        labels.append(class_indices[dirname])\n",
    "        absolute_path = os.path.join(root, fname)\n",
    "        relative_path = os.path.join(\n",
    "                dirname, os.path.relpath(absolute_path, directory))\n",
    "        filenames.append(relative_path)\n",
    "    return filenames, labels\n",
    "        \n",
    "\n",
    "def check_validation_split_arg(validation_split, subset, shuffle, seed):\n",
    "    if validation_split and not 0 < validation_split < 1:\n",
    "        raise ValueError(\n",
    "                '`validation_split` must be between 0 and 1, received: %s' %\n",
    "                (validation_split,))\n",
    "    if (validation_split or subset) and not (validation_split and subset):\n",
    "        raise ValueError(\n",
    "                'If `subset` is set, `validation_split` must be set, and inversely.')\n",
    "    if subset not in ('training', 'validation', None):\n",
    "        raise ValueError('`subset` must be either \"training\" '\n",
    "                         'or \"validation\", received: %s' % (subset,))\n",
    "    if validation_split and shuffle and seed is None:\n",
    "        raise ValueError(\n",
    "                'If using `validation_split` and shuffling the data, you must provide '\n",
    "                'a `seed` argument, to make sure that there is no overlap between the '\n",
    "                'training and validation subset.')\n",
    "\n",
    "\n",
    "def get_training_or_validation_split(samples, labels, validation_split, subset):\n",
    "    if not validation_split:\n",
    "        return samples, labels\n",
    "\n",
    "    num_val_samples = int(validation_split * len(samples))\n",
    "    if subset == 'training':\n",
    "        print('Using %d files for training.' % (len(samples) - num_val_samples,))\n",
    "        samples = samples[:-num_val_samples]\n",
    "        labels = labels[:-num_val_samples]\n",
    "    elif subset == 'validation':\n",
    "        print('Using %d files for validation.' % (num_val_samples,))\n",
    "        samples = samples[-num_val_samples:]\n",
    "        labels = labels[-num_val_samples:]\n",
    "    else:\n",
    "        raise ValueError('`subset` must be either \"training\" '\n",
    "                         'or \"validation\", received: %s' % (subset,))\n",
    "    return samples, labels\n",
    "\n",
    "\n",
    "def labels_to_dataset(labels, label_mode, num_classes):\n",
    "    label_ds = dataset_ops.Dataset.from_tensor_slices(labels)\n",
    "    if label_mode == 'binary':\n",
    "        label_ds = label_ds.map(\n",
    "                lambda x: array_ops.expand_dims(math_ops.cast(x, 'float32'), axis=-1))\n",
    "    elif label_mode == 'categorical':\n",
    "        label_ds = label_ds.map(lambda x: array_ops.one_hot(x, num_classes))\n",
    "    return label_ds\n",
    "\n",
    "\n",
    "def paths_and_labels_to_dataset(file_paths,\n",
    "                                labels,\n",
    "                                label_mode,\n",
    "                                num_classes,\n",
    "                                max_length):\n",
    "    path_ds = dataset_ops.Dataset.from_tensor_slices(file_paths)\n",
    "    string_ds = path_ds.map(\n",
    "        lambda x: path_to_string_content(x, max_length))\n",
    "    if label_mode:\n",
    "        label_ds = labels_to_dataset(labels, label_mode, num_classes)\n",
    "        string_ds = dataset_ops.Dataset.zip((string_ds, label_ds))\n",
    "    return string_ds\n",
    "\n",
    "\n",
    "def path_to_string_content(path, max_length):\n",
    "    txt = io_ops.read_file(path)\n",
    "    if max_length is not None:\n",
    "        txt = string_ops.substr(txt, 0, max_length)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def text_dataset_from_directory(directory,\n",
    "                                labels='inferred',\n",
    "                                label_mode='int',\n",
    "                                class_names=None,\n",
    "                                batch_size=32,\n",
    "                                max_length=None,\n",
    "                                shuffle=True,\n",
    "                                seed=None,\n",
    "                                validation_split=None,\n",
    "                                subset=None,\n",
    "                                follow_links=False):\n",
    "    if labels != 'inferred':\n",
    "        if not isinstance(labels, (list, tuple)):\n",
    "            raise ValueError(\n",
    "                    '`labels` argument should be a list/tuple of integer labels, of '\n",
    "                    'the same size as the number of text files in the target '\n",
    "                    'directory. If you wish to infer the labels from the subdirectory '\n",
    "                    'names in the target directory, pass `labels=\"inferred\"`. '\n",
    "                    'If you wish to get a dataset that only contains text samples '\n",
    "                    '(no labels), pass `labels=None`.')\n",
    "        if class_names:\n",
    "            raise ValueError('You can only pass `class_names` if the labels are '\n",
    "                             'inferred from the subdirectory names in the target '\n",
    "                             'directory (`labels=\"inferred\"`).')\n",
    "    if label_mode not in {'int', 'categorical', 'binary', None}:\n",
    "        raise ValueError(\n",
    "                '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n",
    "                'or None. Received: %s' % (label_mode,))\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(1e6)\n",
    "    check_validation_split_arg(\n",
    "        validation_split, subset, shuffle, seed)\n",
    "    \n",
    "    file_paths, labels, class_names = index_directory(\n",
    "            directory,\n",
    "            labels,\n",
    "            formats=('.txt',),\n",
    "            class_names=class_names,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            follow_links=follow_links)\n",
    "\n",
    "    if label_mode == 'binary' and len(class_names) != 2:\n",
    "        raise ValueError(\n",
    "                'When passing `label_mode=\"binary\", there must exactly 2 classes. '\n",
    "                'Found the following classes: %s' % (class_names,))\n",
    "\n",
    "    file_paths, labels = get_training_or_validation_split(\n",
    "        file_paths, labels, validation_split, subset)\n",
    "\n",
    "    dataset = paths_and_labels_to_dataset(\n",
    "        file_paths=file_paths,\n",
    "        labels=labels,\n",
    "        label_mode=label_mode,\n",
    "        num_classes=len(class_names),\n",
    "        max_length=max_length)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset.class_names = class_names\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "seed = 20200721\n",
    "\n",
    "print('학습 데이터 세트를 불러옵니다.')\n",
    "raw_train_ds = text_dataset_from_directory(\n",
    "        train_dir, \n",
    "        batch_size=batch_size, \n",
    "        validation_split=validation_split, \n",
    "        subset='training',\n",
    "        seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러온 데이터 확인\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(\"Review\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('레이블 번호 - 이름 확인!')\n",
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('검증 데이터 세트를 불러옵니다.')\n",
    "raw_val_ds = text_dataset_from_directory(\n",
    "        train_dir, \n",
    "        batch_size=batch_size, \n",
    "        validation_split=validation_split, \n",
    "        subset='validation', \n",
    "        seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 학습 데이터 준비: 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                    '[%s]' % re.escape(string.punctuation),\n",
    "                                    '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "sequence_length = 50\n",
    "\n",
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_features,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('테스트 데이터와 레이블을 분리합니다.')\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('학습 완료 후 사용할 테스트 세트를 불러옵니다.')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "raw_test_ds = text_dataset_from_directory(\n",
    "        test_dir, \n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 학습 데이터 준비: 전처리 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('테스트 데이터를 확인합니다.')\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(tf.expand_dims(first_review, -1), first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('각 특징 벡터의 원소 값을 확인할 수 있습니다.')\n",
    "voc_size = len(vectorize_layer.get_vocabulary())\n",
    "print(f'Vocabulary size: {voc_size}')\n",
    "base = 0\n",
    "cnt = 10\n",
    "if voc_size - cnt < base:\n",
    "    print(f'{voc_size-cnt} 보다는 작은 수를 입력해야 합니다.') \n",
    "else:\n",
    "    for i in range(base, base+cnt):\n",
    "        print(f'{i+2:4d} ---> {vectorize_layer.get_vocabulary()[i].decode(\"utf-8\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 학습 모델 정의 및 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('학습, 검증, 테스트 데이터를 모두 벡터화 합니다.')\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "print('데이터 입력부를 최적화합니다.')\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4\n",
    "\n",
    "print('모델을 정의합니다.')\n",
    "model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Embedding(max_features+1, embedding_dim, mask_zero=True),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('모델을 준비합니다.')\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "              optimizer=tf.keras.optimizers.Adam(), \n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('모델 성능을 테스트합니다.')\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "fig1 = plt.figure(figsize=(6, 10))\n",
    "ax = fig1.add_subplot(2, 1, 1)\n",
    "ax.plot(epochs, loss, 'bo', label='학습 손실')\n",
    "ax.plot(epochs, val_loss, 'b', label='검증 손실')\n",
    "ax.set_ylim((0, 1))\n",
    "ax.set_title('학습 및 검증 손실', fontproperties=nanum_prop, fontsize=12)\n",
    "ax.set_xlabel('Epochs', fontproperties=nanum_prop, fontsize=10)\n",
    "ax.set_ylabel('손실', fontproperties=nanum_prop, fontsize=10)\n",
    "ax.legend(prop=nanum_prop)\n",
    "\n",
    "ax = fig1.add_subplot(2, 1, 2)\n",
    "ax.plot(epochs, acc, 'bo', label='학습 정확도')\n",
    "ax.plot(epochs, val_acc, 'b', label='검증 정확도')\n",
    "ax.set_ylim((0, 1))\n",
    "ax.set_title('학습 및 검증 정확도', fontproperties=nanum_prop, fontsize=12)\n",
    "ax.set_xlabel('Epochs', fontproperties=nanum_prop, fontsize=10)\n",
    "ax.set_ylabel('정확도', fontproperties=nanum_prop, fontsize=10)\n",
    "ax.legend(prop=nanum_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "fig1 = plt.figure(figsize=(6, 10))\n",
    "ax = fig1.add_subplot(2, 1, 1)\n",
    "ax.plot(epochs, loss, 'bo', label='Train loss')\n",
    "ax.plot(epochs, val_loss, 'b', label='Val loss')\n",
    "# ax.set_ylim((0, 1))\n",
    "ax.set_title('Train and val loss',fontsize=12)\n",
    "ax.set_xlabel('Epochs', fontsize=10)\n",
    "ax.set_ylabel('loss', fontsize=10)\n",
    "ax.legend()\n",
    "\n",
    "ax = fig1.add_subplot(2, 1, 2)\n",
    "ax.plot(epochs, acc, 'bo', label='Train accuracy')\n",
    "ax.plot(epochs, val_acc, 'b', label='Val accuracy')\n",
    "# ax.set_ylim((0, 1))\n",
    "ax.set_title('Train and val accuracy', fontsize=12)\n",
    "ax.set_xlabel('Epochs', fontsize=10)\n",
    "ax.set_ylabel('Accuracy', fontsize=10)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 예측: 가지고 놀기! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('모델 출력')\n",
    "export_model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    model,\n",
    "    tf.keras.layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_str = input('테스트 입력: ').strip()\n",
    "predicted = export_model.predict(tf.expand_dims(user_str, -1))[0][0]\n",
    "print(f'예측 결과: {raw_train_ds.class_names[int(round(predicted))]} ({predicted:0.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
